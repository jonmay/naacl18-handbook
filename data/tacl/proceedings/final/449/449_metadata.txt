SubmissionNumber#=%=#449
FinalPaperTitle#=%=#Learning Structured Text Representations (TACL)
ShortPaperTitle#=%=#
NumberOfPages#=%=#
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.
Author{1}{Firstname}#=%=#Yang
Author{1}{Lastname}#=%=#Liu
Author{1}{Email}#=%=#yang.liu2@ed.ac.uk
Author{1}{Affiliation}#=%=#University of Edinburgh
Author{2}{Firstname}#=%=#Mirella
Author{2}{Lastname}#=%=#Lapata
Author{2}{Email}#=%=#mlap@inf.ed.ac.uk
Author{2}{Affiliation}#=%=#School of Informatics, University of Edinburgh

==========